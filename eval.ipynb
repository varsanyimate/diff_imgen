{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5641bd-3f02-43fd-b8ba-0c33ce0ab58b",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "### Generated Image Quality Metrics\n",
    "\n",
    "1. **FID (Fr√©chet Inception Distance)**\n",
    "   - Measures how similar the distribution of generated images is to real images\n",
    "   - Lower is better (good models: <50, excellent models: <20)\n",
    "   - Uses Inception-v3 features to compare real and generated image statistics\n",
    "\n",
    "2. **Inception Score (IS)**\n",
    "   - Measures both quality and diversity of generated images\n",
    "   - Higher is better (good models: >3, excellent models: >7)\n",
    "   - Evaluates if images contain clear, recognizable objects and are diverse\n",
    "\n",
    "3. **Kernel Inception Distance (KID)**\n",
    "   - Similar to FID but more reliable for smaller sample sizes\n",
    "   - Lower is better (good models: <0.05, excellent models: <0.02)\n",
    "   - Less sensitive to sample size than FID, good for quick evaluations\n",
    "\n",
    "4. **Diversity Score**\n",
    "   - Custom metric measuring average L2 distance between pairs of generated images\n",
    "   - Higher values indicate more diverse outputs\n",
    "   - Helps detect mode collapse (when model generates very similar images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "29af029b-564c-4b2b-a270-1e04110de10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchmetrics.image.kid import KernelInceptionDistance\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch import nn, einsum\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "from model_unet import *\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, device='cuda'):\n",
    "        print(\"Initializing evaluation metrics...\")\n",
    "        self.device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(self.device)\n",
    "        print(self.device)\n",
    "        self.fid = FrechetInceptionDistance(normalize=True).to(device)\n",
    "        self.inception_score = InceptionScore(normalize=True).to(device)\n",
    "        self.kid = KernelInceptionDistance(normalize=True, subset_size=50).to(device)\n",
    "        print(\"‚úì Metrics initialized successfully\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_images(self, model, num_images, image_size=64, batch_size=32):\n",
    "        # Ensure model is in evaluation mode and on the correct device\n",
    "        model.eval()\n",
    "        model = model.to(self.device)\n",
    "        \n",
    "        # Split into batches if needed\n",
    "        batches = num_to_groups(num_images, batch_size)\n",
    "        print(batches)\n",
    "        \n",
    "        # Sample images for each batch\n",
    "        all_images_list = []\n",
    "        for n in batches:\n",
    "            # Generate images for this batch\n",
    "            # Ensure the shape tensor is on the same device as the model\n",
    "            batch_images = sample(model, image_size, diffusion_params=diffusion_params, batch_size=32, channels=3)\n",
    "            \n",
    "            # Take the last image in the sampling process (fully denoised)\n",
    "            batch_images = batch_images[-1].to(self.device)\n",
    "            all_images_list.append(batch_images)\n",
    "        \n",
    "        # Concatenate images\n",
    "        all_images = torch.cat(all_images_list, dim=0)\n",
    "        \n",
    "        # Move to CPU for saving\n",
    "        all_images = all_images.cpu()\n",
    "        \n",
    "        # Normalize to [0, 1] range\n",
    "        all_images = (all_images + 1) * 0.5\n",
    "        \n",
    "        # Ensure images are on CPU and clipped to [0, 1]\n",
    "        all_images = torch.clamp(all_images, 0, 1)\n",
    "        \n",
    "        # Save images\n",
    "        #save_image(all_images, 'generated_images.png', nrow=8)\n",
    "        \n",
    "        return all_images\n",
    "\n",
    "\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def evaluate_samples(self, real_dataloader, model, num_samples=100, batch_size=32):\n",
    "        \"\"\"\n",
    "        Enhanced evaluation with KID score and progress logging\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nüìä Starting evaluation with {num_samples} samples...\")\n",
    "    \n",
    "        print(\"\\n1Ô∏è‚É£ Collecting real images...\")\n",
    "        real_images = []\n",
    "        for batch, _ in real_dataloader:\n",
    "            real_images.append(batch)\n",
    "            if len(torch.cat(real_images)) >= num_samples:\n",
    "                break\n",
    "        real_images = torch.cat(real_images)[:num_samples].to(self.device)\n",
    "        print(f\"‚úì Collected {len(real_images)} real images\")\n",
    "\n",
    "        print(\"\\n2Ô∏è‚É£ Generating samples...\")\n",
    "        generated_images = self.generate_images(model, num_images=num_samples, image_size=64)\n",
    "        generated_images = generated_images.to(self.device)\n",
    "        print(f\"‚úì Generated {len(generated_images)} images\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {}\n",
    "        print(\"\\n3Ô∏è‚É£ Computing metrics...\")\n",
    "        \n",
    "        print(\"Computing FID score...\")\n",
    "        self.fid.reset()\n",
    "        self.fid.update(real_images, real=True)\n",
    "        self.fid.update(generated_images, real=False)\n",
    "        metrics['fid'] = self.fid.compute().item()\n",
    "        print(f\"‚úì FID Score: {metrics['fid']:.2f}\")\n",
    "        \n",
    "        print(\"\\nComputing Inception Score...\")\n",
    "        self.inception_score.reset()\n",
    "        self.inception_score.update(generated_images)\n",
    "        is_mean, is_std = self.inception_score.compute()\n",
    "        metrics['inception_score_mean'] = is_mean.item()\n",
    "        metrics['inception_score_std'] = is_std.item()\n",
    "        print(f\"‚úì Inception Score: {metrics['inception_score_mean']:.2f} ¬± {metrics['inception_score_std']:.2f}\")\n",
    "        \n",
    "        print(\"\\nComputing KID score...\")\n",
    "        self.kid.reset()\n",
    "        self.kid.update(real_images, real=True)\n",
    "        self.kid.update(generated_images, real=False)\n",
    "        kid_mean, kid_std = self.kid.compute()\n",
    "        metrics['kid_mean'] = kid_mean.item()\n",
    "        metrics['kid_std'] = kid_std.item()\n",
    "        print(f\"‚úì KID Score: {metrics['kid_mean']:.4f} ¬± {metrics['kid_std']:.4f}\")\n",
    "        \n",
    "        print(\"\\nComputing diversity score...\")\n",
    "        if len(generated_images) >= 2:\n",
    "            diversity_score = self.calculate_diversity(\n",
    "                generated_images, \n",
    "                num_pairs=min(100, num_samples * 2)\n",
    "            )\n",
    "            metrics['diversity_score'] = diversity_score\n",
    "            print(f\"‚úì Diversity Score: {metrics['diversity_score']:.2f}\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\n‚ú® Evaluation completed in {elapsed_time:.2f} seconds\")\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def calculate_diversity(self, images, num_pairs=100):\n",
    "        \"\"\"\n",
    "        Calculate diversity score with random pairs\n",
    "        \"\"\"\n",
    "        num_images = len(images)\n",
    "        idx1 = torch.randint(0, num_images, (num_pairs,))\n",
    "        idx2 = torch.randint(0, num_images, (num_pairs,))\n",
    "        \n",
    "        images_flat = images.view(len(images), -1)\n",
    "        distances = torch.norm(\n",
    "            images_flat[idx1] - images_flat[idx2], \n",
    "            dim=1, \n",
    "            p=2\n",
    "        )\n",
    "        \n",
    "        return distances.mean().item()\n",
    "\n",
    "def evaluate_diffusion(model, test_loader, num_samples=100):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation with all metrics\n",
    "    \"\"\"\n",
    "    evaluator = Evaluator()\n",
    "    metrics = evaluator.evaluate_samples(\n",
    "        test_loader,\n",
    "        model,\n",
    "        num_samples=num_samples\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìà Final Evaluation Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"FID Score: {metrics['fid']:.2f}\")\n",
    "    print(f\"Inception Score: {metrics['inception_score_mean']:.2f} ¬± {metrics['inception_score_std']:.2f}\")\n",
    "    print(f\"KID Score: {metrics['kid_mean']:.4f} ¬± {metrics['kid_std']:.4f}\")\n",
    "    print(f\"Diversity Score: {metrics['diversity_score']:.2f}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Provide some context about the scores\n",
    "    print(\"\\nüìù Quick interpretation:\")\n",
    "    print(\"FID: Lower is better (good: <50, excellent: <20)\")\n",
    "    print(\"IS: Higher is better (good: >3, excellent: >7)\")\n",
    "    print(\"KID: Lower is better (good: <0.05, excellent: <0.02)\")\n",
    "    print(\"Diversity: Higher indicates more varied outputs\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2798b866-aca0-4702-bf56-101fac4ccc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(image_size, channels,path):\n",
    "    model = Unet(\n",
    "        dim=image_size,\n",
    "        channels=channels,\n",
    "        dim_mults=(1, 2, 4,)\n",
    "    )\n",
    "    \n",
    "    checkpoint = torch.load(path)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def prepare_data_loaders(train_data, train_labels, val_data, val_labels, test_data, test_labels, batch_size):\n",
    "    train_dataset = TensorDataset(train_data, train_labels)\n",
    "    val_dataset = TensorDataset(val_data, val_labels)\n",
    "    test_dataset = TensorDataset(test_data, test_labels)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def load_flowers(batch_size):\n",
    "    test_data = torch.load(\"data/prepared_datasets/train_flowers.pt\")\n",
    "    test_labels = torch.load(\"data/prepared_datasets/train_flowers_labels.pt\")\n",
    "    val_data = torch.load(\"data/prepared_datasets/val_flowers.pt\")\n",
    "    val_labels = torch.load(\"data/prepared_datasets/val_flowers_labels.pt\")\n",
    "    train_data = torch.load(\"data/prepared_datasets/test_flowers.pt\")\n",
    "    train_labels = torch.load(\"data/prepared_datasets/test_flowers_labels.pt\")\n",
    "    train_data = (train_data - train_data.min()) / (train_data.max() - train_data.min())\n",
    "    val_data = (val_data - val_data.min()) / (val_data.max() - val_data.min())\n",
    "    test_data = (test_data - test_data.min()) / (test_data.max() - test_data.min())\n",
    "    \n",
    "    train_data = train_data * 2 - 1\n",
    "    val_data = val_data * 2 - 1\n",
    "    test_data = test_data * 2 - 1\n",
    "    \n",
    "    train_loader, val_loader, test_loader = prepare_data_loaders(train_data, train_labels, val_data, val_labels, test_data, test_labels,batch_size)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f726011-4da2-409e-ae81-b77504b10344",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(64,3,'models/model_3.pth')\n",
    "train_loader, val_loader, test_loader = load_flowers(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5f3d80b-d27b-4edc-b525-04f823abe509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing evaluation metrics...\n",
      "cuda\n",
      "‚úì Metrics initialized successfully\n",
      "\n",
      "üìä Starting evaluation with 128 samples...\n",
      "\n",
      "1Ô∏è‚É£ Collecting real images...\n",
      "‚úì Collected 128 real images\n",
      "\n",
      "2Ô∏è‚É£ Generating samples...\n",
      "[32, 32, 32, 32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:19<00:00, 10.06it/s]\n",
      "sampling loop time step: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:19<00:00, 10.12it/s]\n",
      "sampling loop time step: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:20<00:00, 10.00it/s]\n",
      "sampling loop time step: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:20<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated 128 images\n",
      "\n",
      "3Ô∏è‚É£ Computing metrics...\n",
      "Computing FID score...\n",
      "‚úì FID Score: 293.59\n",
      "\n",
      "Computing Inception Score...\n",
      "‚úì Inception Score: 1.41 ¬± 0.09\n",
      "\n",
      "Computing KID score...\n",
      "‚úì KID Score: 0.3057 ¬± 0.0142\n",
      "\n",
      "Computing diversity score...\n",
      "‚úì Diversity Score: 18.28\n",
      "\n",
      "‚ú® Evaluation completed in 83.56 seconds\n",
      "\n",
      "üìà Final Evaluation Results:\n",
      "========================================\n",
      "FID Score: 293.59\n",
      "Inception Score: 1.41 ¬± 0.09\n",
      "KID Score: 0.3057 ¬± 0.0142\n",
      "Diversity Score: 18.28\n",
      "========================================\n",
      "\n",
      "üìù Quick interpretation:\n",
      "FID: Lower is better (good: <50, excellent: <20)\n",
      "IS: Higher is better (good: >3, excellent: >7)\n",
      "KID: Lower is better (good: <0.05, excellent: <0.02)\n",
      "Diversity: Higher indicates more varied outputs\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_diffusion(\n",
    "    model,\n",
    "    test_loader,\n",
    "    num_samples=128\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
