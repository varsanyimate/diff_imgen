{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9a2806-7c07-452c-8909-ff02059df1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/gradio/utils.py:999: UserWarning: Expected 2 arguments for function <function generate_images_and_gif at 0x7f0dce19a680>, received 1.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/gradio/utils.py:1003: UserWarning: Expected at least 2 arguments for function <function generate_images_and_gif at 0x7f0dce19a680>, received 1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://0c558aa2952083fdc5.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0c558aa2952083fdc5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/gradio/helpers.py:965: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
      "/opt/conda/lib/python3.10/site-packages/gradio/helpers.py:965: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
      "/opt/conda/lib/python3.10/site-packages/gradio/helpers.py:965: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import torch\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from src.model_unet_no_attention import Unet as Unet_No_Att\n",
    "from src.model_unet import *\n",
    "from src.eval_base import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def load_model(image_size, channels,path):\n",
    "    if \"NoAttention\" in path:\n",
    "        model = Unet_No_Att(\n",
    "                dim=image_size,\n",
    "                channels=channels,\n",
    "                dim_mults=(1, 2, 4,)\n",
    "            )\n",
    "    elif \"noattention\" in path:\n",
    "        model = Unet_No_Att(\n",
    "                dim=image_size,\n",
    "                channels=channels,\n",
    "                dim_mults=(1, 2, 4,)\n",
    "            ) \n",
    "    elif \"resnet\" in path:\n",
    "        model = Unet(\n",
    "                dim=image_size,\n",
    "                channels=channels,\n",
    "                dim_mults=(1, 2, 4,),\n",
    "                use_convnext=False\n",
    "            )\n",
    "    else:\n",
    "        model = Unet(\n",
    "            dim=image_size,\n",
    "            channels=channels,\n",
    "            dim_mults=(1, 2, 4,)\n",
    "        )\n",
    "    \n",
    "    checkpoint = torch.load(path)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_images_and_gif(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Generate images and create a GIF using the specified model and dataset\n",
    "    \"\"\"\n",
    "    # Load model and dataset\n",
    "    img_size=64\n",
    "    model = load_model(64,3,f'../models/{model_name}')\n",
    "    timesteps = 200\n",
    "    betas = DiffusionSchedule.linear_beta_schedule(timesteps).clone()\n",
    "    diffusion_params = DiffusionSchedule.compute_diffusion_parameters(betas)\n",
    "    samples = sample(model, image_size=img_size,diffusion_params=diffusion_params, batch_size=32, channels=3)\n",
    "    images = samples[-1][:9]\n",
    "    # Generate samples (replace with your actual generation method)\n",
    "   #eval_base = Evaluator(model=model)\n",
    "   #generated_images = eval_base.generate_images(num_images=9, image_size=6)\n",
    "   #generated_images = generated_images.to(device)\n",
    "    #samples = fast_generate_images(model, num_images=9, image_size=256)\n",
    "    \n",
    "    # Create grid of final images\n",
    "    grid_image = create_image_grid(samples,img_size)\n",
    "    \n",
    "    # Create GIF\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')  # Turn off axes\n",
    "    fig.patch.set_alpha(0)  # Remove background\n",
    "    ims = []\n",
    "    \n",
    "    \n",
    "    for i in range(200):\n",
    "        img = samples[i][1]\n",
    "        img = img.cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0)) \n",
    "        #img = samples[i][random_index].reshape(image_size, image_size, channels)\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        im = plt.imshow(img, animated=True)\n",
    "        ims.append([im])\n",
    "    \n",
    "    animate = animation.ArtistAnimation(fig, ims, interval=5, blit=True, repeat_delay=1000)\n",
    "    gif_path= '../gifs/diffusion.gif'\n",
    "    animate.save(gif_path)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return grid_image, gif_path\n",
    "\n",
    "def create_image_grid(samples, img_size, padding=5, bg_color=0.9):\n",
    "    \"\"\"\n",
    "    Create a grid of generated images with padding\n",
    "    \n",
    "    Args:\n",
    "        samples: Generated image samples\n",
    "        img_size: Size of each image\n",
    "        padding: Pixel width of padding between images\n",
    "        bg_color: Background color for padding (0.5 is neutral gray)\n",
    "    \"\"\"\n",
    "    # Convert samples to numpy for visualization\n",
    "    samples_np = samples[-1].cpu().numpy()\n",
    "    \n",
    "    # Create a grid of 9 images (3x3)\n",
    "    grid_size = 3\n",
    "    \n",
    "    # Calculate total grid size with padding\n",
    "    total_size = img_size * grid_size + padding * (grid_size + 1)\n",
    "    grid_image = np.full((total_size, total_size, 3), bg_color)\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            idx = i * grid_size + j\n",
    "            img = samples_np[idx]\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "            \n",
    "            # Calculate positioning with padding\n",
    "            start_y = padding + i * (img_size + padding)\n",
    "            start_x = padding + j * (img_size + padding)\n",
    "            \n",
    "            grid_image[\n",
    "                start_y:start_y+img_size, \n",
    "                start_x:start_x+img_size\n",
    "            ] = img\n",
    "    \n",
    "    return grid_image\n",
    "\n",
    "def create_gradio_interface():\n",
    "    # List available models and datasets\n",
    "    import os\n",
    "\n",
    "    models = [f for f in os.listdir('../models') if os.path.isfile(os.path.join('../models', f)) and f.endswith('.pth')]\n",
    "\n",
    "    #datasets = ['flowers', 'celeba']\n",
    "    \n",
    "    # Create Gradio interface\n",
    "    with gr.Blocks() as demo:\n",
    "        with gr.Row():\n",
    "            model_dropdown = gr.Dropdown(choices=models, label=\"Select Model\")\n",
    "            #dataset_dropdown = gr.Dropdown(choices=datasets, label=\"Select Dataset\")\n",
    "        \n",
    "        generate_btn = gr.Button(\"Generate Images\")\n",
    "        \n",
    "        grid_output = gr.Image(label=\"Image Grid\")\n",
    "        gif_output = gr.Image(label=\"Diffusion GIF\")\n",
    "        img_size =64\n",
    "        generate_btn.click(\n",
    "            fn=generate_images_and_gif, \n",
    "            inputs=[model_dropdown],\n",
    "            outputs=[grid_output, gif_output]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    interface = create_gradio_interface()\n",
    "    interface.launch(share=True, allowed_paths=[\"/app/gifs\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
