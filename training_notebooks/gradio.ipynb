{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9a2806-7c07-452c-8909-ff02059df1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://6af2ab2ce20be5d3c6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6af2ab2ce20be5d3c6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ampling loop time step: 100%|██████████| 200/200 [00:20<00:00,  9.95it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import torch\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from src.model_unet_no_attention import Unet as Unet_No_Att\n",
    "from src.model_unet import *\n",
    "from src.eval_base import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def load_model(image_size, channels,path):\n",
    "    if \"NoAttention\" in path:\n",
    "        model = Unet_No_Att(\n",
    "                dim=image_size,\n",
    "                channels=channels,\n",
    "                dim_mults=(1, 2, 4,)\n",
    "            )\n",
    "    else:\n",
    "        model = Unet(\n",
    "            dim=image_size,\n",
    "            channels=channels,\n",
    "            dim_mults=(1, 2, 4,)\n",
    "        )\n",
    "    \n",
    "    checkpoint = torch.load(path)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_data_loaders(train_data, train_labels, val_data, val_labels, test_data, test_labels, batch_size):\n",
    "    train_dataset = TensorDataset(train_data, train_labels)\n",
    "    val_dataset = TensorDataset(val_data, val_labels)\n",
    "    test_dataset = TensorDataset(test_data, test_labels)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def load_dataset(dataset_name,batch_size):\n",
    "    \"\"\"\n",
    "    Load dataset based on selected dataset name\n",
    "    \"\"\"\n",
    "    if dataset_name == \"flowers\":\n",
    "        test_data = torch.load(\"../data/prepared_datasets/train_flowers.pt\")\n",
    "        test_labels = torch.load(\"../data/prepared_datasets/train_flowers_labels.pt\")\n",
    "        val_data = torch.load(\"../data/prepared_datasets/val_flowers.pt\")\n",
    "        val_labels = torch.load(\"../data/prepared_datasets/val_flowers_labels.pt\")\n",
    "        train_data = torch.load(\"../data/prepared_datasets/test_flowers.pt\")\n",
    "        train_labels = torch.load(\"../data/prepared_datasets/test_flowers_labels.pt\")\n",
    "    elif dataset_name == \"celeba\":\n",
    "        test_data = torch.load(\"../data/prepared_datasets/train_celeba.pt\")\n",
    "        test_labels = torch.load(\"../data/prepared_datasets/train_celeba_labels.pt\")\n",
    "        val_data = torch.load(\"../data/prepared_datasets/val_celeba.pt\")\n",
    "        val_labels = torch.load(\"../data/prepared_datasets/val_celeba_labels.pt\")\n",
    "        train_data = torch.load(\"../data/prepared_datasets/test_celeba.pt\")\n",
    "        train_labels = torch.load(\"../data/prepared_datasets/test_celeba_labels.pt\")\n",
    "    train_data = (train_data - train_data.min()) / (train_data.max() - train_data.min())\n",
    "    val_data = (val_data - val_data.min()) / (val_data.max() - val_data.min())\n",
    "    test_data = (test_data - test_data.min()) / (test_data.max() - test_data.min())\n",
    "    \n",
    "    train_data = train_data * 2 - 1\n",
    "    val_data = val_data * 2 - 1\n",
    "    test_data = test_data * 2 - 1\n",
    "    \n",
    "    train_loader, val_loader, test_loader = prepare_data_loaders(train_data, train_labels, val_data, val_labels, test_data, test_labels,batch_size)\n",
    "    return train_loader, val_loader, test_loader\n",
    "    #return dataset_name\n",
    "\n",
    "def generate_images_and_gif(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Generate images and create a GIF using the specified model and dataset\n",
    "    \"\"\"\n",
    "    # Load model and dataset\n",
    "    img_size=64\n",
    "    model = load_model(64,3,f'../models/{model_name}')\n",
    "    train_loader, val_loader, test_loader = load_dataset(dataset_name,32)\n",
    "    timesteps = 200\n",
    "    betas = DiffusionSchedule.linear_beta_schedule(timesteps).clone()\n",
    "    diffusion_params = DiffusionSchedule.compute_diffusion_parameters(betas)\n",
    "    samples = sample(model, image_size=img_size,diffusion_params=diffusion_params, batch_size=32, channels=3)\n",
    "    images = samples[-1][:9]\n",
    "    # Generate samples (replace with your actual generation method)\n",
    "   #eval_base = Evaluator(model=model)\n",
    "   #generated_images = eval_base.generate_images(num_images=9, image_size=6)\n",
    "   #generated_images = generated_images.to(device)\n",
    "    #samples = fast_generate_images(model, num_images=9, image_size=256)\n",
    "    \n",
    "    # Create grid of final images\n",
    "    grid_image = create_image_grid(samples,img_size)\n",
    "    \n",
    "    # Create GIF\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')  # Turn off axes\n",
    "    fig.patch.set_alpha(0)  # Remove background\n",
    "    ims = []\n",
    "    \n",
    "    \n",
    "    for i in range(200):\n",
    "        img = samples[i][1]\n",
    "        img = img.cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0)) \n",
    "        #img = samples[i][random_index].reshape(image_size, image_size, channels)\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        im = plt.imshow(img, animated=True)\n",
    "        ims.append([im])\n",
    "    \n",
    "    animate = animation.ArtistAnimation(fig, ims, interval=5, blit=True, repeat_delay=1000)\n",
    "    gif_path= '../gifs/diffusion.gif'\n",
    "    animate.save(gif_path)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return grid_image, gif_path\n",
    "\n",
    "def create_image_grid(samples, img_size, padding=5, bg_color=0.9):\n",
    "    \"\"\"\n",
    "    Create a grid of generated images with padding\n",
    "    \n",
    "    Args:\n",
    "        samples: Generated image samples\n",
    "        img_size: Size of each image\n",
    "        padding: Pixel width of padding between images\n",
    "        bg_color: Background color for padding (0.5 is neutral gray)\n",
    "    \"\"\"\n",
    "    # Convert samples to numpy for visualization\n",
    "    samples_np = samples[-1].cpu().numpy()\n",
    "    \n",
    "    # Create a grid of 9 images (3x3)\n",
    "    grid_size = 3\n",
    "    \n",
    "    # Calculate total grid size with padding\n",
    "    total_size = img_size * grid_size + padding * (grid_size + 1)\n",
    "    grid_image = np.full((total_size, total_size, 3), bg_color)\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            idx = i * grid_size + j\n",
    "            img = samples_np[idx]\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "            \n",
    "            # Calculate positioning with padding\n",
    "            start_y = padding + i * (img_size + padding)\n",
    "            start_x = padding + j * (img_size + padding)\n",
    "            \n",
    "            grid_image[\n",
    "                start_y:start_y+img_size, \n",
    "                start_x:start_x+img_size\n",
    "            ] = img\n",
    "    \n",
    "    return grid_image\n",
    "\n",
    "def create_gradio_interface():\n",
    "    # List available models and datasets\n",
    "    models = [f for f in os.listdir('../models') if os.path.isfile(os.path.join('../models', f))]\n",
    "    datasets = ['flowers', 'celeba']\n",
    "    \n",
    "    # Create Gradio interface\n",
    "    with gr.Blocks() as demo:\n",
    "        with gr.Row():\n",
    "            model_dropdown = gr.Dropdown(choices=models, label=\"Select Model\")\n",
    "            dataset_dropdown = gr.Dropdown(choices=datasets, label=\"Select Dataset\")\n",
    "        \n",
    "        generate_btn = gr.Button(\"Generate Images\")\n",
    "        \n",
    "        grid_output = gr.Image(label=\"Image Grid\")\n",
    "        gif_output = gr.Image(label=\"Diffusion GIF\")\n",
    "        img_size =64\n",
    "        generate_btn.click(\n",
    "            fn=generate_images_and_gif, \n",
    "            inputs=[model_dropdown, dataset_dropdown],\n",
    "            outputs=[grid_output, gif_output]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    interface = create_gradio_interface()\n",
    "    interface.launch(share=True, allowed_paths=[\"/app/gifs\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
